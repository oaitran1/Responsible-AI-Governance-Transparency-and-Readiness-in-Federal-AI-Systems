Knowledge gap - What is missing from prior research? This is why this research matters. 

Theme - the dominant threads through the literature.

Theme:
###  Research Methods for AI-in-Education Studies

- **Core works:** Braun & Clarke (2006); Creswell (2018); Johnson & Onwuegbuzie (2004); Hsu & Sandford (2007) and UNESCO, OECD, IEEE
- **Focus:** Methodological rigor in mixed-methods, rubric design, and Delphi validation.
- **Trend:** Growing emphasis on combining _quantitative learning analytics_ with _qualitative frameworks_ to understand meaning and context.
- **Tension:** Few frameworks integrate ethical dimensions with analytic design i.e., _Responsible AI methodology_ is underdeveloped.

### Knowledge gap
- While ethical guidelines for AI (e.g., UNESCO, OECD, IEEE) and AI literacy frameworks exist, no integrated model connects these principles to _measurable educational practice_. Current research remains fragmented ethical frameworks lack implementation pathways, and learning analytics often ignore ethical dimensions. This gap highlights the need for a **validated framework**, such as RAIFE, to operationalize Responsible AI through competency-based assessment and ethical design in education.

## Reading
### **1. Introduction**

- **Purpose:** Frame the rise of AI in education (AIEd) and the need for critical reflection on literacy, ethics, and human-centered adoption.
- **Anchor citations:** Zhai et al. (2020); Holmes & Luckin (2016).
- **Goal:** Outline how the literature evolved—from early arguments for AI potential → practical applications → literacy and policy concerns.

---

### **2. Historical Overview and Research Trends**

**Key Source:** _Zhai et al. (2020)_ – _A Review of AI in Education from 2010–2020_
- Summarize trends (adaptive learning, ITS, analytics, affective computing).
- Highlight methods: bibliometric and content analyses showing rapid growth post-2015.
- Knowledge gap: research heavily focused on _technological affordances_ rather than _pedagogical or ethical integration._
---

### **3. Technological Landscape and Future Directions**

**Key Source:** _Zhang & Aslan (2021)_ – _AI technologies for education: Recent research & future directions_
- Map types of AI used in education (machine learning, NLP, computer vision).
- Identify challenges: scalability, bias, explainability.
- Connect to current generative AI trends (bridge to Kasneci et al. 2023).
- Gap: Few longitudinal studies assessing long-term impacts of AI integration.

---

### **4. AI Literacy in K-12 and Higher Education**

**Key Sources:**
- _Maher & Young (2022)_ – _AI and Literacy Development in K-12 Schools_
- _Long & Magerko (2020)_ – _What Is AI Literacy? Competencies and Design Considerations_
- Discuss definitions and frameworks of AI literacy (knowledge, skills, and ethics).
- Highlight design considerations for curriculum integration.
- Gap: lack of standardized, scalable frameworks linking AI literacy to learning outcomes.
---

### **5. Ethical and Policy Dimensions**

**Key Sources:**

- _Schiff (2022)_ – _Education for AI, not AI for Education_
- _Holmes & Luckin (2016)_ – _Intelligence Unleashed: An Argument for AI in Education_
- Focus on: human-in-the-loop ethics, equity, transparency, data privacy.
- Schiff’s argument: education should shape AI ethics and policy, not just consume AI tools.    
- Gap: limited empirical work evaluating ethical frameworks in practice.

---

### **6. Generative AI and Large Language Models**

**Key Source:** _Kasneci et al. (2023)_ – _ChatGPT for good?_
- Summarize opportunities and challenges of LLMs in education (personalization vs. misinformation).
- Introduce the “responsible innovation” and “explainable AI” debates.
- Gap: absence of frameworks integrating LLMs responsibly within curriculum and assessment.

---

### **7. Synthesis and Conceptual Gap**

**Integrate Across Sections:**
- **Knowledge Gaps Identified:**
    1. Lack of unified framework connecting _AI literacy_, _ethical design_, and _pedagogical outcomes_.
    2. Limited empirical and longitudinal studies evaluating the _impact of AI tools and literacy interventions_.
- **Emerging Theme:**  
    Responsible AI adoption in education requires a **multi-layered literacy model**—linking technical understanding, ethical reasoning, and contextual pedagogy (bridge to your RAIFE/AI literacy framework).
---
### **8. Methodological Implications for Your Research**

- Consider **bibliometric analysis** (to map field evolution, as in Zhai et al.).
- Add **Delphi method** or **systematic review synthesis** for gap refinement.
- Possibly include **framework design** or **mixed-methods validation** (if you’re building a model like RAIFE).

---
### **Current**

- What risks, if any, do AI systems show?
- Risk disclosures don't correlate with system impact
- will agencies mention harms or downstream effects
RAIFE
- How will this be structured into risk categories?
- Impact scoring?
- system lifecycle risk mapping
- cross-stakeholder risk evaluation
- This is the bridge:
	- If or is there any inconsistent inventories
	- more standardized, auditable, multi-dimensional evaluation
-
- Federal agencies publicly list AI systems; are they fragmented, uneven, and dependent on agency capacity???
- This leads to transparency gaps, risk blind spots, and inconsistent governance across agencies
- RAIFE provides a structured, multi-dimensional model to close these gaps: literacy, system transparency, governance maturity, risk mapping, and accountability
- Stage 1 empirical results empirically justify RAIFE:
    - They reveal the baseline
    - They expose the specific deficiencies RAIFE is designed to fix
    - They help define RAIFE’s dimensions and rubric
- The RAIFE output matrix becomes the operational tool that agencies—not just researchers—can adopt to move toward responsible AI governance



2.1 Literature Review

Artificial intelligence (AI) literacy has emerged as a foundational requirement for responsible AI integration in education. Early frameworks such as [1] highlight competency-based approaches that define what learners and educators must under- stand to interact safely and effectively with AI systems. Classical perspectives on intelligent tutoring systems [] also provide useful historical grounding for thinking about how AI systems structure learning interactions.

Recent work extends these foundations. Long and Magerko [2] identify core competen- cies for AI literacy, arguing that both conceptual and socio-technical understanding are essential. Research in K–12 contexts [] shows that AI literacy is unevenly developed across schools and educational systems, with significant challenges around educator preparedness. Likewise, Zhang and Aslan [3] survey AI applications in education and point to the growing need for transparent models and human-centered design.

Beyond literacy, policy frameworks are gaining momentum. UNESCO’s 2021 Rec- ommendation on the Ethics of AI [] provides global guidance for human-centered, rights-based AI governance. These principles inform modern educational AI systems and support the argument for structured evaluation frameworks such as RAIFE. The broader educational ecosystem is also evolving rapidly, with more recent texts [] highlighting how leaders plan to integrate AI across classrooms and administrative systems.

Responsible AI Governance in the Public Sector

Responsible AI frameworks from the OECD (2019), UNESCO (2021), NIST (2023), and IEEE (2019) emphasize transparency, fairness, accountability, and human over- sight, yet they remain largely at the principle level and offer limited pathways for operationalizing governance in public institutions. Floridi and Cowls’ (2019) five ethical principles, beneficence, non-maleficence, autonomy, justice, and explicability, provide a normative foundation, but public agencies continue to struggle with trans- lating these ideals into procurement, risk assessment, documentation, and day-to-day oversight.

Administrative law scholars argue that algorithmic systems challenge traditional accountability structures by obscuring responsibility and reducing contestability (Veale & Brass, 2019). Selbst et al. (2019) show that abstraction errors, the removal of systems from their real institutional contexts, produce governance failures. Raji et al. (2022) further demonstrate that internal audits are insufficient, calling for ecosystem- level third-party oversight to evaluate public-sector AI. These insights collectively argue for context-sensitive, institution-specific frameworks that move beyond broad ethical principles.

AI Literacy & Human-Centered Governance

5

A parallel body of work highlights a critical literacy gap among policymakers and public administrators. Long and Magerko (2021) define AI literacy as the knowledge, skills, and attitudes needed to meaningfully govern AI-mediated systems. Ng et al. (2021) argue that the public sector requires a distinct form of AI literacy, focused on oversight, risk interpretation, and accountability rather than technical development. Shneiderman (2020) advances the concept of “human-centered AI,” emphasizing that non-technical actors must be able to understand and interrogate system behavior to ensure democratic control. This literature suggests that governance failures often stem not from malicious design but from capacity deficits within public institutions.

Transparency, System Evaluation, and Governance Maturity

Transparency tools such as accountable algorithms (Kroll et al., 2017), model docu- mentation (Mitchell et al., 2019), and dataset disclosures (Gebru et al., 2021) offer mechanisms for evaluating system behavior. Edwards and Veale (2017) warn, how- ever, that many automated systems cannot be adequately governed through existing legal and administrative processes. The U.S. Government Accountability Office (GAO) repeatedly identifies gaps in agencies’ AI inventories, documentation practices, risk assessments, and oversight plans, revealing a mismatch between policy expectations and institutional readiness. Wirtz and Mu ̈ller (2019) extend this argument by framing AI adoption in government as a socio-technical process requiring organizational capa- bility, data maturity, and public-value alignment. Their work shows that governance quality depends not only on system performance but on the maturity of institutional processes supporting transparency, evaluation, and accountability.

These lay the foundation for this paper; however, they reveal a deeper challenge: the federal government lacks a shared evaluation and literacy framework for responsible AI adoption. Agencies disclose what they can articulate, not what is required for con- sistent governance. Critical dimensions of responsible AI risk mitigation, downstream impact, data provenance, fairness considerations, human oversight, and accountability mechanisms are often missing or only loosely referenced. The absence of a stan- dardized reporting structure means that transparency is unevenly distributed across government, creating a governance gap with direct implications for equity, public accountability, and system safety.