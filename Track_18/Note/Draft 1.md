
# **Connecting the Transparency Study to RAIFE: A Coherent Research Narrative**

_(You can drop this directly into your paper.)_

Public-sector organizations are under increasing pressure to disclose how artificial intelligence (AI) systems are designed, deployed, and governed. The federal AI use-case inventories mandated under Executive Order 13960 were created to raise transparency and provide a consistent view of how agencies use AI across mission areas. However, the inventories remain largely descriptive and vary substantially across agencies in depth, specificity, and quality. Agencies differ not only in what they disclose, but also in the clarity with which they communicate data sources, model design, risks, and governance practices.

This project provides the first system-level analysis of transparency across the federal AI inventory. By combining the consolidated AI use-case listings with agency-level characteristics—such as resource capacity, workforce composition, governance maturity, and external oversight—we establish a quantitative baseline of the current state of disclosure. The analysis shows where transparency is strong, where it is inconsistent, and where it is structurally limited by agency capacity. Agencies with higher technical staffing, clearer AI governance structures, or external oversight tend to provide richer and more structured disclosures, while resource-constrained agencies report significantly less detail.

These findings reveal a deeper challenge: _the federal government has no shared evaluation or literacy framework for responsible AI adoption._ Agencies disclose what they are able to articulate, not what is required for consistent governance. Critical dimensions of responsible AI—risk mitigation, downstream impact, data provenance, fairness considerations, human oversight, and accountability mechanisms—are often missing or only loosely referenced. The absence of a standardized reporting structure means that transparency is unevenly distributed across government, creating a governance gap with direct implications for equity, public accountability, and system safety.

This gap directly motivates the Responsible AI Framework for Education (RAIFE), which we present as a conceptual next step rather than a replacement for existing inventories. RAIFE offers a structured, multi-dimensional evaluation model that formalizes what agencies should disclose—not simply what they currently choose or are able to disclose. Where the transparency analysis reveals inconsistent reporting, RAIFE introduces standardized dimensions; where disclosures omit risk considerations, RAIFE provides a coherent risk-mapping structure; where workforce capacity predicts disclosure quality, RAIFE defines the literacy requirements necessary for meaningful evaluation.

In this way, the transparency study establishes both the baseline and the need for RAIFE. The empirical results show _what exists_; the conceptual framework shows _what is missing_ and _how to address it._ RAIFE is therefore positioned as a governance tool that operationalizes responsible AI principles through a practical, evaluative rubric. It provides agencies with a structured way to map system characteristics to public-value principles, document risks, articulate accountability structures, and support literacy among system developers, implementers, and policymakers.

Framed together, the transparency analysis and RAIFE represent complementary stages of a broader research agenda: diagnosing the current state of AI governance in the public sector, identifying the structural gaps, and developing a standardized approach to fill those gaps. The transparency study reveals the limits of existing disclosure practices; RAIFE offers a coherent model to move beyond these limits toward accountable, interpretable, and equitable AI adoption in government.