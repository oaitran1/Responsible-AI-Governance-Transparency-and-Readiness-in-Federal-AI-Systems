# Summary

- Why is this important?
- What is the theme and the knowledge gap?
	- Theme and Knowledge gap --> check [[02 Literature Review]]
	- 
- What are we doing? 
	- [[01 Project plans and Method]]
	- Alg
- How do we measure success? 
	- ???

## Funding

[[Funding]] here will be list of possible funding

## Project Plan

- [ ] Phase 1:
	- [ ] Define the research questions
	- [ ] Collect and clean data 
		- [ ] Current stage 
		- [ ] Knowledge gaps
		- [ ] Data Methodology- How are we filtering raw data. Check [[03 Data Source]] and 
		- [ ] Create Train and Test

- [ ] Phase 2:
	- [ ] Model methodology
		- [ ] Load and fine tone model/s
		- [ ] Compute values
		- [ ] Validate and compare model/s --> compare differences between models and rank them
	- [ ] Additional analysis 
		- [ ] On Results
		- [ ] Quantify  --> what does the values mean, and what threshold is valuable 

- [ ] Phase 3
	- [ ] How do we validate our results:
		- [ ] Case studies --> manually inspect clusters and see if career flow matches real-world
		- [ ] Visualization check

- [ ] Phase 4
	- [ ] Model Robustness and Extension Application
		- [ ] Use Out-of-Time (OOT) or new data sources to check and see if results are validated

- [ ] Phase 5
	- [ ] Manual script write-up 
	- [ ] Code review 
	- [ ] Data audit --> pull sample and revalidate the process
	- [ ] Documentations gather
	- [ ] Push to the Main branch

- Plan:
current stage --> RAIFE --> Outputs

RQ1. How transparent are U.S. federal agencies about the design, data, and risks of the AI systems they deploy, and what agency characteristics predict higher transparency?
- Uses the U.S. Federal AI Use Case Inventory (public)
- Transparency can be quantified from system descriptions
- Predictors: mission area, domain, agency size, model type
- Very clear dependent variable: “transparency score.”


